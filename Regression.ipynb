{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhN7n9H7ylyvuiYw2B7sJR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oxrH2KQMWeEp"},"outputs":[],"source":["#1. What is Simple Linear Regression?\n","'''\n","Simple Linear Regression is a statistical method used to model the relationship between two continuous variables.\n","It aims to find a straight line that best fits the data points, where one variable (the independent variable) is\n","used to predict the other (the dependent variable).\n","'''\n","#2. What are the key assumptions of Simple Linear Regression?\n","'''\n","The key assumptions are:\n","\n","Linearity: The relationship between the independent and dependent variables is linear.\n","Independence: Observations are independent of each other.\n","Homoscedasticity: The variance of the errors (residuals) is constant across all levels of the independent variable.\n","Normality: The errors (residuals) are normally distributed.\n","No Autocorrelation: Residuals are independent of each other.\n","'''\n","#3. What does the coefficient m represent in the equation Y=mX+c?\n","'''\n","In the equation Y=mX+c, the coefficient m represents the slope of the regression line.\n","It indicates how much the dependent variable (Y) is expected to change for every one-unit increase in the independent variable (X).\n","'''\n","#4. What does the intercept c represent in the equation Y=mX+c?\n","'''\n","In the equation Y=mX+c, the intercept c represents the Y-intercept. It is the predicted value of the dependent variable (Y)\n","when the independent variable (X) is equal to zero. It provides a baseline value for Y.\n","'''\n","#5. How do we calculate the slope m in Simple Linear Regression?\n","'''\n","The slope m in Simple Linear Regression is calculated using the formula:\n","m=(y2-y1)/(x2-x1)\n","\n","where y1 and x1 are the first data points, and y2 and x2 are the second data points.\n","'''\n","#6.What is the purpose of the least squares method in Simple Linear Regression?\n","'''\n","The purpose of the least squares method is to find the line that minimizes the sum of the squared differences\n","between the observed values of the dependent variable and the values predicted by the regression line.\n","It finds the \"best fit\" line by reducing the overall error.\n","'''\n","#7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n","'''\n","The coefficient of determination (R²) is a value between 0 and 1 (or 0% and 100%). It represents the proportion of the variance\n","in the dependent variable that is predictable from the independent variable.\n","\n","An R² of 0 means the model explains none of the variance in Y.\n","An R² of 1 means the model explains all of the variance in Y.\n","An R² of, say, 0.75 means that 75% of the variability in the dependent variable is explained by the independent variable in the model.\n","'''\n","#8. What is Multiple Linear Regression?\n","'''\n","Multiple Linear Regression is an extension of Simple Linear Regression where two or more independent variables are\n","used to predict a single continuous dependent variable. The model aims to find a linear equation that best describes the relationship between\n","the dependent variable and the set of independent variables.\n","'''\n","#9.What is the main difference between Simple and Multiple Linear Regression?\n","'''\n","The main difference lies in the number of independent variables used:\n","\n","Simple Linear Regression: Uses one independent variable to predict the dependent variable.\n","Multiple Linear Regression: Uses two or more independent variables to predict the dependent variable.\n","'''\n","#10.What are the key assumptions of Multiple Linear Regression?\n","'''\n","The key assumptions for Multiple Linear Regression are similar to Simple Linear Regression, with additional considerations:\n","\n","Linearity: The relationship between the dependent variable and each independent variable is linear.\n","Independence: Observations are independent of each other.\n","Homoscedasticity: The variance of the errors (residuals) is constant across all combinations of independent variables.\n","Normality: The errors (residuals) are normally distributed.\n","No Multicollinearity: Independent variables are not highly correlated with each other.\n","No Autocorrelation: Residuals are independent of each other.\n","'''\n","#11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n","'''\n","Heteroscedasticity occurs when the variance of the errors (residuals) is not constant across all levels of the independent variables.\n","In simpler terms,the spread of the residuals is wider for some values of the independent variables than for others.\n","\n","It affects the results by:\n","\n","Making the standard errors of the coefficients biased, which leads to\n","incorrect conclusions about the statistical significance of the independent variables.\n","Making the regression coefficients less efficient (less precise).\n","Violating one of the key assumptions of linear regression, potentially leading to unreliable inferences.\n","'''\n","#12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n","'''\n","Multicollinearity occurs when independent variables are highly correlated with each other.\n","This makes it difficult to determine the individual effect of each independent variable on the dependent variable.\n","\n","To improve a model with high multicollinearity, you can:\n","\n","Remove one of the highly correlated variables: If two variables are measuring similar things, removing one might solve the issue.\n","Combine highly correlated variables: Create a new variable that is a composite of the correlated variables.\n","Use dimensionality reduction techniques: Methods like Principal Component Analysis (PCA) can reduce\n","the number of independent variables while retaining most of the information.\n","Collect more data: Increasing the sample size can sometimes help to reduce the impact of multicollinearity.\n","'''\n","#13. What are some common techniques for transforming categorical variables for use in regression models?\n","'''\n","Categorical variables need to be converted into numerical representations for use in regression models. Common techniques include:\n","\n","One-Hot Encoding: Creates binary (0 or 1) dummy variables for each category. This is suitable for nominal categorical variables where\n","there is no inherent order.Label Encoding: Assigns a unique integer to each category. This is suitable for ordinal categorical\n","variables where there is an order.However, it can introduce an artificial order if the variable is nominal, which might not be desirable for some models.\n","Effect Encoding (or Sum Coding): Similar to dummy coding but compares each category to the overall mean.\n","Binary Encoding: Converts categories into binary code.\n","'''\n","#14. What is the role of interaction terms in Multiple Linear Regression?\n","'''\n","Interaction terms are included in a Multiple Linear Regression model to assess whether\n","the effect of one independent variable on the dependent variable depends on the level of another independent variable.\n","\n","For example, if you are modeling house prices, an interaction term between \"size\" and \"location\" could show whether\n","the effect of size on price is different in different locations.\n","'''\n","#15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n","'''\n","Simple Linear Regression: The intercept is the predicted value of the dependent variable when the single independent variable is zero.\n","Multiple Linear Regression: The intercept is the predicted value of the dependent variable when all independent variables are zero.\n","This interpretation only makes sense if a value of zero is meaningful and within the range of the data for all independent variables.\n","'''\n","#16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n","'''\n","The slope represents the change in the dependent variable associated with a one-unit increase in the independent variable\n","(holding other independent variables constant in Multiple Regression).\n","\n","Significance: The statistical significance of the slope (determined by p-values) indicates whether there\n","is a statistically meaningful relationship between the independent variable and the dependent variable.\n","A significant slope suggests that the independent variable is a useful predictor.\n","Predictions: The slope is crucial for making predictions. By multiplying the value of the independent variable(s)\n","by their respective slopes and addingthe intercept, the model predicts the value of the dependent variable.\n","'''\n","#17.How does the intercept in a regression model provide context for the relationship between variables?\n","'''\n","The intercept provides a baseline value for the dependent variable when the independent variable(s) are at their zero point.\n","It helps to understand the starting point of the relationship. However, its interpretation is only meaningful if zero is a valid\n","and observed value for the independent variables. In some cases, the intercept might not have a practical interpretation,\n","but it is still necessary for the mathematical formulation of the line of best fit.\n","'''\n","#18.What are the limitations of using R² as a sole measure of model performance?\n","'''\n","While R² is a useful metric, it has limitations as a sole measure of model performance:\n","\n","Doesn't indicate causality: A high R² doesn't mean the independent variables cause the changes in the dependent variable.\n","Increases with more variables: R² will always increase or stay the same when you add more independent variables to the model,\n","even if those variables are not good predictors. This can lead to overfitting. Doesn't assess the validity of assumptions:\n","A high R² doesn't guarantee that the model's assumptions (linearity, homoscedasticity, etc.) are met.\n","Doesn't indicate prediction accuracy for new data: A high R² on the training data doesn't guarantee good performance on unseen data.\n","'''\n","#19. How would you interpret a large standard error for a regression coefficient?\n","'''\n","A large standard error for a regression coefficient indicates that the estimate\n","of the coefficient is less precise and has higher variability. This can mean:\n","\n","The effect of the independent variable on the dependent variable is uncertain.\n","The coefficient might not be statistically significant (a large standard error can lead to a higher p-value).\n","The sample size might be too small to accurately estimate the coefficient.\n","There might be multicollinearity issues.\n","'''\n","#20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n","'''\n","Heteroscedasticity can be identified in residual plots by observing a pattern where the spread of the residuals\n","changes across the values of the independent variable(s) or the predicted values. A common pattern is a fanning-out or fanning-in shape.\n","\n","It is important to address heteroscedasticity because it violates the assumption of constant variance, leading to:\n","\n","Biased standard errors and incorrect p-values, affecting statistical inference.\n","Less efficient coefficient estimates.\n","Unreliable confidence intervals and hypothesis tests.\n","Addressing it often involves transforming the dependent variable, using weighted least squares, or using robust standard errors.\n","'''\n","#21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n","'''\n","If a Multiple Linear Regression model has a high R² but a significantly lower adjusted R², it suggests that\n","the model includes independent variables that do not contribute meaningfully to explaining the variance in the dependent variable.\n","R²: Increases with every added variable, regardless of its significance.\n","Adjusted R²: Penalizes the addition of unnecessary variables. It only increases if the new variable improves the model more than\n","would be expected by chance.A high R² with a low adjusted R² is a sign of overfitting, where the model is capturing random noise\n","in the data rather than the true underlying relationship.\n","'''\n","#22.Why is it important to scale variables in Multiple Linear Regression?\n","'''\n","Scaling variables in Multiple Linear Regression (especially when using regularization techniques like Ridge or Lasso) is important for several reasons:\n","\n","Equal contribution: It ensures that all independent variables contribute equally to the model, regardless of their original scale.\n","Without scaling, variables with larger scales might dominate the model.\n","Improved optimization: For gradient-based optimization algorithms, scaling can lead to faster convergence.\n","Meaningful regularization: Regularization techniques penalize large coefficients. If variables are not scaled, coefficients for\n","variables with smaller scales might be penalized more heavily, even if their impact is significant.\n","'''\n","#23. What is polynomial regression?\n","'''\n","Polynomial regression is a form of regression analysis in which the relationship between the independent variable (X)\n","and the dependent variable (Y) is modeled as an n-th degree polynomial. It allows for fitting non-linear relationships between variables.\n","'''\n","#24. How does polynomial regression differ from linear regression?\n","'''\n","The key difference is in the functional form of the relationship:\n","\n","Linear Regression: Models the relationship as a straight line (Y = mX + c).\n","Polynomial Regression: Models the relationship as a curve defined by a polynomial equation Y=β0+β1X+β2X^2+..\n","While polynomial regression fits a curve, it is still considered a form of linear regression because it is linear in the coefficients ().\n","'''\n","#25. When is polynomial regression used?\n","'''\n","Polynomial regression is used when the relationship between the independent and dependent variables is clearly non-linear.\n","It is applied when a straight line is not sufficient to capture the pattern in the data. Examples include modeling the trajectory of an\n","object,the growth of a population, or the relationship between temperature and material expansion.\n","'''\n","#26. What is the general equation for polynomial regression?\n","'''\n","The general equation for polynomial regression of degree n is:\n","Y=β0+β1X+β2X^2+...+βnX^n+epsilon\n","\n","where:\n","\n","Y is the dependent variable\n","X is the independent variable\n","beta_0 is the intercept\n","beta_1,beta_2, ... are the coefficients for the polynomial terms\n","epsilon- is the error term\n","'''"]}]}